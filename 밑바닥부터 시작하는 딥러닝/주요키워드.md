## Underfittin & Overfitting

![untitled](C:\Users\multicampus\ssafy08\TIL\밑바닥부터 시작하는 딥러닝\assets\1.png)

- **Underfitting**
  - 훈련 데이터 셋도 아직 학습을 제대로 못한 상태
  - 학습 반복 횟수가 너무 적거나
  - 데이터의 특성에 비해 너무 간단한 모델을 사용 했거나
  - 데이터의 양이 너무 적은 경우
  > 해결방법
  > 1. 학습 반복 횟수 늘리기
  > 2. 모델의 복잡도 높이기(layer 깊이 늘리기)
  > 3. 데이터 추가 수집 혹은 augmentation을 통한 데이터 증대

- **Overfitting**
  - 학습 데이터를 과하게 학습하여,
  - 학습 데이터에는 잘 맞지만 검증 데이터에 잘 맞지 않는 것
  - Underfitting과는 좀 반대로
  - 학습을 과도하게 시켰거나
  - 모델의 복잡도가 데이터에 비해 높거나
  - 데이터의 양이 부족한 경우(한쪽으로 치우친 학습)
  > 해결방법
  > 1. 적절한 시점에 학습 종료시키기
  > 2. 모델의 복잡도 줄이기
  > 3. 데이터 추가 수집 혹은 augmentation을 통한 데이터 증대
  > 4. Dropout 사용하기(학습할 때 신경망의 일부를 사용하지 않음으로 학습 과정마다 다른 결과를 낼 수 있음)
  > 5. Batch Normalization

## Cross Entropy Error







