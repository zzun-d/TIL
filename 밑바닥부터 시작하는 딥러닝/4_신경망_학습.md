# 4. 신경망 학습
---
## 4.1 데이터에서 학습한다!
- 신경망은 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다.
- 데이터는 기계학습의 생명! 데이터가 없으면 패턴을 발견할 수 없고 학습이 진행되지 않음
- 이미지 데이터에서 **특징**을 추출하고 그 특징의 패턴을 기계학습으로 학습이 가능
- 신경망 학습은 사람의 개입이 최소화된다(데이터만 있으면 스스로 특징 파악)
- 이러한 특징에서 딥러닝은 **종단간 기계학습**이라고도 부른다
- 학습에 사용되는 데이터는 훈련 데이터와 시험 데이터로 나눈다
    > - 우선 훈련 데이터만 사용하여 최적의 매개변수를 찾는다
    > - 이후 시험 데이터를 사용하여 훈련한 모델의 성능을 평가한다.
- 훈련 데이터와 시험 데이터를 나누는 이유는?
    > - 훈련시킨 모델이 얼마나 범용적인지 파악하기 위해서
    > - 데이터셋을 나누지 않고 학습을 진행하면 **오버피팅**의 위험이 있음

<br>

## 4.2 손실 함수

- 신경망 학습에서 사용되는 지표로 최적의 매개변수 값을 탐색하기 위한 함수
- 가장 많이 쓰이는 손실함수는 **오차제곱합**으로 수식은 아래와 같다.
    > $$E = {1\over 2}\Sigma(y_k - t_k)^2$$
- 여기서 $y_k$는 신경망의 출력, $t_k$는 정답 레이블, $k$는 데이터의 차원 수를 나타낸다.
- 분류문제에서 **원-핫 인코딩**을 많이 사용하는데, 가장 높은 값을 가지는 원소 하나만 1로 하고 나머지는 0으로 표기하는 방법이다.
```python
# 오차제곱합 함수로 구현
def sum_squares_error(y, t):
    return 0.5 * np.sum((y-t)**2)

# 정답 레이블은 '2'
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# 예1 : '2'일 확률이 가장 높다고 추정(0.6)
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
sum_squares_error(np.array(y), np.array(t))
# 0.0975

# 예2 : '7'일 확률이 가장 높다고 추정(0.6)
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
sum_squares_error(np.array(y), np.array(t))
# 0.5975
```
첫 번째 예는 정답이 2이며, 예측도 2로 했을 경우의 오차제곱합이며,
두 번째 예는 정답은 2지만, 예측은 7로 했을 경우의 오차제곱합이다.
손실 함수의 값이 잘못 예측했을 경우에 훨씬 큰 것을 확인할 수 있다.

<br>


- 또 다른 손실함수로 **교차 엔트로피 오차**가 있으며 수식은 아래와 같다.
    > $$E = - \Sigma{t_klog{y_k}}$$
- 여기서 $log$는 밑이 $e$인 자연로그($log_e$)며, $y_k$는 신경망의 출력, $t_k$는 정답 레이블이다.
- $t_k$는 원-핫 인코딩 되어있다.
```python
# 교차 엔트로피 오차 함수로 구현
def cross_entropy_error(y, t):
    delta = 1e-7    # 아주 작은 수
    return -np.sum(t * np.log(y + delta))   # 아주 작은 수를 더해 log에 0대입을 방지

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))
# 0.5108245709933802

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))
# 2.3025840929945458
```
위 오차제곱합과 같은 레이블과 예측값을 가져왔으며, 결과도 비슷하게 내는 것을 확인할 수 있다.

**왜 손실 함수를 설정하는가?**
> 궁극의 목적은 높은 '정확도'를 끌어내는 매개변수 값을 찾는 것.
신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수를 찾게 된다. 이 과정에서 경사하강법을 사용한다.




<br>
- **미니배치**
    - 훈련 데이터가 일정 수준이 넘어가면 모든 데이터를 이용해서 훈련시켜 손실 함수를 계산하는 것은 쉽지 않다.
    - 이럴경우 일부를 추려 '근사치'로 이용이 가능하다. 이 일부의 데이터를 **미니배치** 라고하며, 미니배치를 이용한 학습을 **미니배치 학습**이라 한다.