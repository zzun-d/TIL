# 4. 신경망 학습
---
## 4.1 데이터에서 학습한다!
- 신경망은 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다.
- 데이터는 기계학습의 생명! 데이터가 없으면 패턴을 발견할 수 없고 학습이 진행되지 않음
- 이미지 데이터에서 **특징**을 추출하고 그 특징의 패턴을 기계학습으로 학습이 가능
- 신경망 학습은 사람의 개입이 최소화된다(데이터만 있으면 스스로 특징 파악)
- 이러한 특징에서 딥러닝은 **종단간 기계학습**이라고도 부른다
- 학습에 사용되는 데이터는 훈련 데이터와 시험 데이터로 나눈다
    > - 우선 훈련 데이터만 사용하여 최적의 매개변수를 찾는다
    > - 이후 시험 데이터를 사용하여 훈련한 모델의 성능을 평가한다.
- 훈련 데이터와 시험 데이터를 나누는 이유는?
    > - 훈련시킨 모델이 얼마나 범용적인지 파악하기 위해서
    > - 데이터셋을 나누지 않고 학습을 진행하면 **오버피팅**의 위험이 있음

<br>

## 4.2 손실 함수

- 신경망 학습에서 사용되는 지표로 최적의 매개변수 값을 탐색하기 위한 함수
- 가장 많이 쓰이는 손실함수는 **오차제곱합**으로 수식은 아래와 같다.
    > $$E = {1\over 2}\Sigma(y_k - t_k)^2$$
- 여기서 $y_k$는 신경망의 출력, $t_k$는 정답 레이블, $k$는 데이터의 차원 수를 나타낸다.
- 분류문제에서 **원-핫 인코딩**을 많이 사용하는데, 가장 높은 값을 가지는 원소 하나만 1로 하고 나머지는 0으로 표기하는 방법이다.
```python
# 오차제곱합 함수로 구현
def sum_squares_error(y, t):
    return 0.5 * np.sum((y-t)**2)

# 정답 레이블은 '2'
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# 예1 : '2'일 확률이 가장 높다고 추정(0.6)
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
sum_squares_error(np.array(y), np.array(t))
# 0.0975

# 예2 : '7'일 확률이 가장 높다고 추정(0.6)
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
sum_squares_error(np.array(y), np.array(t))
# 0.5975
```
첫 번째 예는 정답이 2이며, 예측도 2로 했을 경우의 오차제곱합이며,
두 번째 예는 정답은 2지만, 예측은 7로 했을 경우의 오차제곱합이다.
손실 함수의 값이 잘못 예측했을 경우에 훨씬 큰 것을 확인할 수 있다.

<br>


- 또 다른 손실함수로 **교차 엔트로피 오차**가 있으며 수식은 아래와 같다.
    > $$E = - \Sigma{t_klog{y_k}}$$
- 여기서 $log$는 밑이 $e$인 자연로그($log_e$)며, $y_k$는 신경망의 출력, $t_k$는 정답 레이블이다.
- $t_k$는 원-핫 인코딩 되어있다.
```python
# 교차 엔트로피 오차 함수로 구현
def cross_entropy_error(y, t):
    delta = 1e-7    # 아주 작은 수
    return -np.sum(t * np.log(y + delta))   # 아주 작은 수를 더해 log에 0대입을 방지

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))
# 0.5108245709933802

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))
# 2.3025840929945458
```
위 오차제곱합과 같은 레이블과 예측값을 가져왔으며, 결과도 비슷하게 내는 것을 확인할 수 있다.

**왜 손실 함수를 설정하는가?**
> 궁극의 목적은 높은 '정확도'를 끌어내는 매개변수 값을 찾는 것.
신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수를 찾게 된다. 이 과정에서 경사하강법을 사용한다.


<br>
- **미니배치**
    - 훈련 데이터가 일정 수준이 넘어가면 모든 데이터를 이용해서 훈련시켜 손실 함수를 계산하는 것은 쉽지 않다.
    - 이럴경우 일부를 추려 '근사치'로 이용이 가능하다. 이 일부의 데이터를 **미니배치** 라고하며, 미니배치를 이용한 학습을 **미니배치 학습**이라 한다.

## 4.3 수치 미분

- **미분을 파이썬에서 구현**
$$
{d{f(x)}\over dx} = \lim_{h\to 0}{f(x+h)-f(x)\over h}
$$
```python
def numerical_diff(f, x):
    h = 1e-4    # 아주 작은 값을 표현
    return (f(x+h) - f(x-h)) / (2*h)
```

- **편미분 파이썬에서 구현**
$$
f(x_0, x_1) = x^2_0 + x^2_1
$$
```python
def function_2(x):
    return x[0]**2 + x[1]**2

# x0 = 3, x1 = 4일 때, x0에 대한 편미분
def function_tmp1(x0):
    return x0 * x0 + 4.0**2.0

numerical_diff(function_tmp1, 3.0)
# 6.000000000000378

# x0 = 3, x1 = 4일 때, x1에 대한 편미분
def function_tmp2(x1):
    return 3.0**2.0 + x1 * x1

numerical_diff(function_tmp2, 4.0)
# 7.999999999999119
```

## 4.4 기울기
- 모든 변수의 편미분을 벡터로 정리한 것으로 다음과 같이 구현 가능

    ```python
    def numerical_gradient(f, x):
        h = 1e-4
        grad = np.zeros_like(x)  # x와 형상이 같은 0으로 찬 배열 생성

        for idx in range(x.size):
            tmp_val = x[idx]

            # f(x+h) 계산
            x[idx] = tmp_val + h
            fxh1 = f(x)

            #f(x-h) 계산
            x[idx] = tmp_val - h
            fxh2 = f(x)

            grad[idx] = (fxh1 - fxh2) / (2*h)
            x[idx] = tmp_val    # 값 복원
        
        return grad
    ```
    
### 경사하강법
- 현재 위치에서 기울어진 방향으로 일정 거리만큼 이동하는 것을 반복하여 함수의 극솟값으로 접근하는 방법을 경사하강법이라 한다.
```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x

    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x
```
f는 최적화 하고자 하는 목적 함수, init_x는 초기값, lr은 learning rate로 학습률, step_num은 반복 횟수를 뜻한다.
함수의 기울기를 numerical_gradient(f, x)로 구한다음, 그 기울기에 학습률을 곱한 값으로 갱신하는 과정을 step_num만큼 반복한다.

- 연습문제
- 경사하강법으로 $f(x_0,x_1) = x^2_0 + x^2_1$의 최솟값을 구하라.
```python
def function_2(x):
    return x[0]**2 + x[1]**2

init_x = np.array([-3.0, 4.0])
gradient_descent(function_2, init_x, 0.1, 100)
```
- **하이퍼파라미터**
    - 학습률과 같은 매개변수를 하이퍼파라미터라고 한다. 신경망의 매개변수는 자동으로 학습되는 반면, 학습률은 사람이 직접 설정해야 하는 매개변수이다. 보통 이 값은 여러 실험을 통해서 최적의 값을 찾아야 한다.

## 4.5 학습 알고리즘 구현 정리!!
- **1단계 미니배치**
    - 훈련 데이터 중 일부를 무작위로 가져온다(데이터의 양이 많을 경우 overfitting방지 및 학습 시간 줄이기)
- **2단계 기울기 산출**
    - 미니배치의 손실 함수 값을 줄이기 위한 각 가중치 매개변수의 기울기를 구한다.
- **3단계 매개변수 갱신**
    - 2단계에서 구한 가중치 매개변수의 기울기를 매개변수에 빼줌으로써 매개변수를 갱신한다.
    - lr(학습률) 설정!!
- **4단계 반복**
    - 1~3단계를 반복한다.
    - step_num

- 미니배치를 무작위로 가져오기 때문에 **확률적 경사 하강법**이라 부른다
- 확률적 경사 하강법은 훈련 시간을 줄여줄 뿐 아니라 local minima에 빠지는 문제도 일반 경사 하강법에 비해 적다고 알려져 있다.
